from typing import Optional, List, Union, Dict, Tuple
from dataclasses import dataclass, field

from numpy.typing import NDArray

from reports.metrics import ModelMetricsGenerator, MultipleModelsReportGenerator


@dataclass
class ModelReportData:
    """
    A data class for generating and storing metrics related to a single model's performance.

    Args:
        ground_truth (NDArray): An array containing the actual labels (ground truth) for the model predictions.
        predictions (NDArray): An array containing the predicted probabilities or labels generated by the model.
        tag (Optional[str]): An optional tag to identify the model (e.g., 'random', 'const').
        threshold (float): The threshold value for converting predicted probabilities into binary labels (default is 0.5).
        t1d_ratio (float): The prevalence or prior probability of the positive class (default is 0.1).
    """

    ground_truth: NDArray
    predictions: NDArray
    tag: Optional[str]
    threshold: float = field(default=0.5)
    t1d_ratio: float = field(default=0.1)

    def __post_init__(self):
        """
        Initializes the `ModelMetricsGenerator` instance to calculare model metrics.
        """
        self.metrics_generator = ModelMetricsGenerator(
            self.ground_truth,
            self.predictions,
            threshold=self.threshold,
            tag=self.tag,
            prevalence=self.t1d_ratio,
        )

    @property
    def metrics(self) -> Dict[str, float]:
        """
        Computes and returns the model's performance metrics.

        Returns:
            Dict[str, float]: A dictionary with calculated metrics.
        """
        return self.metrics_generator.get_metrics()


@dataclass
class MultipleModelsReportData:
    """
    A data class for generating comparative reports for multiple models.

    Args:
        report_data (ModelReportData): The report data object containing the metrics for the primary model.
        report_data_random (ModelReportData): The report data object containing the metrics for the random model.
        report_data_const (ModelReportData): The report data object containing the metrics for the constant model.
        model_tags (List): A list of tags identifying each model in the comparison (e.g., [None, 'random', 'const']).

    """

    report_data: ModelReportData
    report_data_random: ModelReportData
    report_data_const: ModelReportData

    model_tags: List

    def __post_init__(self):
        """
        Initializes the `MultipleModelsReportGenerator` instance to compute comparative metrics.
        """
        self.metrics_generator = MultipleModelsReportGenerator(
            self.report_data,
            self.report_data_random,
            self.report_data_const,
        )

    @property
    def models_comparison_metrics(self) -> Dict[str, Union[Dict, Tuple[NDArray]]]:
        """
        Computes and returns the comparison metrics for all models.

        Return:
            Dict[str, Union[Dict, Tuple[NDArray]]]: A dictionary containing comparison metrics.
        """
        return self.metrics_generator.get_metrics()

    def individual_model_metrics(self, tag: Optional[str] = None) -> Dict[str, float]:
        """
        Retrieves the metrics for an individual model based on its tag.

        Args:
            tag (Optional[str]): An optional tag identifying the model (e.g., 'random', 'const').


        Returns:
            Dict[str, float]: A dictionary with calculated metrics for specific model.
        """
        metrics = {
            "random": self.report_data_random.metrics,
            "const": self.report_data_const.metrics,
        }

        return metrics.get(tag, self.report_data.metrics)

    @property
    def metrics(self) -> List[Dict[str, float]]:
        """
        Computes and returns the metrics for all individual models specified in `model_tags`.

        Returns:
            List[Dict[str, float]]: A list of dictionaries, each containing the metrics for an individual model.
        """
        metrics = list()
        for tag in self.model_tags:
            metric = self.individual_model_metrics(tag)
            metrics.append(metric)

        return metrics
